{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -i https://test.pypi.org/simple/ adawat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import itertools\n",
    "from adawat.nlp import Corpus\n",
    "\n",
    "corpus_path='/home/rafid/Downloads/wiki-sents/wiki-sents.txt'\n",
    "#corpus_path='../input/wikipedia-sentences/wikisent2.txt'\\\n",
    "#corpus_path='./text8'\n",
    "\n",
    "def to_lowercase(line: str) -> str:\n",
    "    return line.lower()\n",
    "\n",
    "#MAX_LINES = 100_000\n",
    "#MAX_VOCAB = 1_000\n",
    "#corpus = Corpus(corpus_path, MAX_LINES, MAX_VOCAB, preprocessor=to_lowercase)\n",
    "\n",
    "#MAX_VOCAB = 30_000\n",
    "\n",
    "#with gzip.open(\"/home/rafid/Workspace/tmikolov-word2vec/dataset/news.2012.en.shuffled.normalized.gz\") as f:\n",
    "#    lines_as_string = map(lambda l: str(l), f)\n",
    "#    corpus = Corpus(lines_as_string, min_word_freq=5, force_init=True)\n",
    "with open(corpus_path, \"r\") as f:\n",
    "    corpus = Corpus(f, min_word_freq=5, force_init=True)\n",
    "\n",
    "len(corpus.tokens), corpus.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import itertools\n",
    "from adawat.utils import GeneratorWithLen\n",
    "from functools import partial\n",
    "\n",
    "CONTEXT_RADIUS = 5\n",
    "\n",
    "def build_training_data(corpus: Corpus, context_radius=2) -> Tuple:\n",
    "    length = 0\n",
    "    for tokens in corpus.tokens_per_line:\n",
    "        length += (len(tokens) - context_radius) - context_radius\n",
    "\n",
    "    \n",
    "    def build_features(corpus: Corpus, context_radius=2):\n",
    "        for tokens in corpus.tokens_per_line:\n",
    "            for i in range(context_radius, len(tokens) - context_radius):\n",
    "                yield tokens[i - context_radius:i] + tokens[i + 1: i + context_radius + 1]\n",
    "\n",
    "    def build_targets(corpus: Corpus, context_radius=2):\n",
    "        for tokens in corpus.tokens_per_line:\n",
    "            for i in range(context_radius, len(tokens) - context_radius):\n",
    "                yield tokens[i]\n",
    "    \n",
    "    return (GeneratorWithLen(partial(build_features, corpus, context_radius), length),\n",
    "        GeneratorWithLen(partial(build_targets, corpus, context_radius), length))\n",
    "\n",
    "\n",
    "features, targets = build_training_data(corpus, CONTEXT_RADIUS)\n",
    "\n",
    "list(zip(itertools.islice(features, 3), itertools.islice(targets, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, context_radius, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(2 * context_radius * embedding_dim, 512)\n",
    "        self.linear2 = nn.Linear(512, vocab_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output1 = self.embedding(input)\n",
    "        output1 = torch.flatten(output1, 1)\n",
    "        output2 = F.relu(self.linear1(output1))\n",
    "        output3 = self.linear2(output2)\n",
    "\n",
    "        return F.log_softmax(output3, dim=1)\n",
    "\n",
    "\n",
    "def model_func(vocab_size, context_radius, embedding_dim):\n",
    "    \"\"\"\n",
    "     A function for defining instances of the model for use with the ModelTrainer class\n",
    "    \"\"\"\n",
    "\n",
    "    def model():\n",
    "        return CBOW(vocab_size, context_radius, embedding_dim)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_model():\n",
    "    model_fn = model_func(corpus.vocab_size, CONTEXT_RADIUS, EMBEDDING_DIM)\n",
    "    model = model_fn()\n",
    "    output = model(torch.tensor(\n",
    "        [list(range(0, 2*CONTEXT_RADIUS))] * 10\n",
    "    ))\n",
    "    print(output)\n",
    "    print(output.shape)\n",
    "\n",
    "\n",
    "test_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! Using it.\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"CUDA is not available :'( Using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "from itertools import islice\n",
    "\n",
    "# PyTorch impots\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 3rd party imports\n",
    "from adawat.data import IterableDataset\n",
    "from adawat.transforms import Compose, WordToIndex, WordsToIndices, \\\n",
    "    WordToOneHot, ToPyTorchTensor\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "dataset = IterableDataset(\n",
    "    features, targets,\n",
    "    # Features transformation\n",
    "    Compose(\n",
    "        WordsToIndices(corpus.word2idx),\n",
    "        ToPyTorchTensor(dtype=torch.long, device=device)\n",
    "    ),\n",
    "    # Targets transformation\n",
    "    Compose(\n",
    "        #WordToOneHot(corpus.word2idx, corpus.vocab_size),\n",
    "        WordToIndex(corpus.word2idx),\n",
    "        ToPyTorchTensor(dtype=torch.long, device=device)\n",
    "    ),\n",
    "    len(features) # this should be the same as len(targets)\n",
    ")\n",
    "data_loader = DataLoader(dataset, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "def configure_logging():\n",
    "    import logging\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.DEBUG)\n",
    "\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "\n",
    "    root.handlers = [ch]\n",
    "configure_logging()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch imports\n",
    "from torch import optim\n",
    "\n",
    "# Other imports\n",
    "from adawat.models import ModelTrainer\n",
    "\n",
    "\n",
    "def closest_vector(vector, vectors):\n",
    "    vectors = np.asarray(vectors)\n",
    "    dist_2 = np.sum((vectors - vector)**2, axis=1)\n",
    "    dist_2_sorted = sorted(zip(dist_2, range(len(dist_2))))\n",
    "    return dist_2_sorted[1][1]\n",
    "\n",
    "\n",
    "def get_word_embedding(embedding_matrix, word, word2idx):\n",
    "    idx = word2idx(word)\n",
    "    return embedding_matrix(torch.tensor(idx, device=device))\n",
    "\n",
    "\n",
    "def train():\n",
    "    lr_start = 0.001\n",
    "    lr_end = 0.00001\n",
    "    def optim_creator(parameters):\n",
    "        return optim.SGD(parameters, lr_start)\n",
    "\n",
    "    epoch_count = 3    \n",
    "    def optim_updater(optim, epoch, i, iter_count):\n",
    "        abs_index = epoch*iter_count + i\n",
    "        last_index = epoch_count*iter_count - 1\n",
    "        new_lr = lr_start + (lr_end - lr_start)*abs_index/last_index\n",
    "        #print(f'Changing learning rate to {new_lr}')\n",
    "        for g in optim.param_groups:\n",
    "            g['lr'] = new_lr\n",
    "\n",
    "    model_fn = model_func(corpus.vocab_size, CONTEXT_RADIUS, EMBEDDING_DIM)\n",
    "    model_trainer = ModelTrainer(model_fn, nn.NLLLoss, optim_creator, optim_updater)\n",
    "    model, losses = model_trainer.train(data_loader, epoch_count, device)\n",
    "    embedding = model.embedding\n",
    "    embedding_matrix = model.embedding.weight\n",
    "    \n",
    "    for word in random.choices(corpus.vocab, k=100):\n",
    "        word_vector = get_word_embedding(embedding, word, corpus.word2idx)\n",
    "        closest_idx = closest_vector(word_vector.detach().cpu().numpy(),\n",
    "                                 embedding_matrix.detach().cpu().numpy())\n",
    "        print(f\"The closest word to {word} is {corpus.idx2word(closest_idx)}\")\n",
    "    \n",
    "    return model, losses\n",
    "\n",
    "\n",
    "train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}